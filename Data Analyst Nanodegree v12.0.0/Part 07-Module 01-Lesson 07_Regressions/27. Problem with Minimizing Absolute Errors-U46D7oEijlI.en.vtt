WEBVTT
Kind: captions
Language: en

00:00:00.590 --> 00:00:03.880
Now you've thought about that let me show you why this matters.

00:00:03.880 --> 00:00:06.340
Let me take the example of the middle line first.

00:00:06.340 --> 00:00:08.650
My errors are going to look something like this.

00:00:08.650 --> 00:00:09.710
I'm sketching them in right now.

00:00:09.710 --> 00:00:13.070
It's just the distance from each point to sort of the predicted value for

00:00:13.070 --> 00:00:15.790
that point on the regression line.

00:00:15.790 --> 00:00:19.370
And if you were just summing the absolute value of the errors you

00:00:19.370 --> 00:00:22.930
would just sum up all of these orange distances and you would have your answer.

00:00:22.930 --> 00:00:24.740
But now let's look at the top line.

00:00:24.740 --> 00:00:26.870
We can do the same thing here too.

00:00:26.870 --> 00:00:28.920
So now we start drawing in the distances.

00:00:28.920 --> 00:00:30.970
So while we'll be closer to all the points above it.

00:00:30.970 --> 00:00:33.820
It would be further away from all of the points below it.

00:00:33.820 --> 00:00:37.130
So taking these two points as concrete examples.

00:00:37.130 --> 00:00:38.610
It'll be a little bit closer to the top point and

00:00:38.610 --> 00:00:40.660
a little bit further away from the bottom point.

00:00:40.660 --> 00:00:42.150
But overall the total error for

00:00:42.150 --> 00:00:45.640
these two example points would be exactly the same.

00:00:45.640 --> 00:00:48.520
As a total error for these two points to the middle line.

00:00:48.520 --> 00:00:50.030
And, in fact, the same thing would be true for

00:00:50.030 --> 00:00:52.030
the bottom regression line as well.

00:00:52.030 --> 00:00:54.200
And if you have equal number of points above and

00:00:54.200 --> 00:00:57.510
below each of these lines then, in general, that's always going to be true.

00:00:57.510 --> 00:01:01.910
There's a fundamental ambiguity when you use the absolute value of

00:01:01.910 --> 00:01:06.290
the errors in terms of exactly where the regression can fall.

00:01:06.290 --> 00:01:07.470
It could be anywhere in this range.

00:01:08.850 --> 00:01:13.830
In other words, there can be multiple lines that minimize the absolute errors.

00:01:13.830 --> 00:01:18.310
There's only going to be one line that minimizes the error squared.

00:01:18.310 --> 00:01:22.890
In other words, this ambiguity does not exist when the metric that we use is not

00:01:22.890 --> 00:01:26.300
the absolute value of the distance, the absolute value of the distance squared.

00:01:27.300 --> 00:01:29.590
There's one more thing that I should add as well.

00:01:29.590 --> 00:01:32.150
This is a little bit more of a practical concern.

00:01:32.150 --> 00:01:35.340
And that is this using the sum of the squared error as a way of

00:01:35.340 --> 00:01:38.630
finding the regression also makes the implementation underneath the hood of

00:01:38.630 --> 00:01:40.420
the regression much easier.

00:01:40.420 --> 00:01:43.100
It's much easier to find this line when what you're trying to do is minimize

00:01:43.100 --> 00:01:46.680
the sum of the squared errors instead of the sum of the absolute errors.

00:01:46.680 --> 00:01:50.180
And this is something we have the luxury of not worrying about too much

00:01:50.180 --> 00:01:54.370
when we're using sklearn to do most of the computational heavy lifting for us.

00:01:54.370 --> 00:01:57.450
But of course, if you're actually writing the code that goes in and

00:01:57.450 --> 00:02:00.400
does the linear algebra to find the regression, or

00:02:00.400 --> 00:02:05.530
maybe the calculus to find the results of what your regression answer should be,

00:02:05.530 --> 00:02:07.450
then this is a big concern to you.

00:02:07.450 --> 00:02:10.258
This is another reason why traditionally regressions are going to

00:02:10.258 --> 00:02:11.342
be minimizing the sum of the squared error.

00:02:11.884 --> 00:02:14.263
Is because that computationally it's just much easier to do that.

