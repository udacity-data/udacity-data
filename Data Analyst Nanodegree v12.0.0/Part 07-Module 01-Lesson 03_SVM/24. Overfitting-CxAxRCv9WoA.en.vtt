WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:03.260
&gt;&gt; So, we just saw an example where the decision boundary was really,

00:00:03.260 --> 00:00:04.840
really complicated.

00:00:04.840 --> 00:00:06.920
So, we have a situation like this.

00:00:06.920 --> 00:00:11.390
We call this overfitting and it's a common phenomena in machine learning that

00:00:11.390 --> 00:00:15.140
you have to be always aware of every time we do machine learning.

00:00:15.140 --> 00:00:18.700
To give an example, if you look at the data over here,

00:00:18.700 --> 00:00:20.090
it seems to be linearly separable.

00:00:20.090 --> 00:00:24.770
A decision surface between red and blue, it looks like this.

00:00:24.770 --> 00:00:30.880
Correctly classifies the red data, but looks erratic at many other places.

00:00:30.880 --> 00:00:32.930
That happens when you take your data too literal,

00:00:32.930 --> 00:00:37.160
and when your machine learning algorithm produces something as complex this,

00:00:37.160 --> 00:00:43.430
as opposed to something as simple as this you are over-fitting.

00:00:43.430 --> 00:00:48.340
So, in machine learning we really want to avoid over-fitting.

00:00:48.340 --> 00:00:51.960
&gt;&gt; One of the ways that you can control over-fitting is through the parameter of

00:00:51.960 --> 00:00:53.110
your algorithm.

00:00:53.110 --> 00:00:55.880
So, in SVM we have several parameters we've talked about.

00:00:55.880 --> 00:00:58.750
C, gamma and the kernel come to mind.

00:00:58.750 --> 00:01:02.210
So, which of these do you think affect the over-fitting potential of an SVM?

00:01:03.360 --> 00:01:05.170
Click as many as you think might be correct.

