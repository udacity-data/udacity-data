WEBVTT
Kind: captions
Language: en

00:00:00.130 --> 00:00:03.610
So we learned a lot about outliers and outlier rejection.

00:00:03.610 --> 00:00:06.760
Rejection is what you're going to do to clean up your fit, but

00:00:06.760 --> 00:00:11.030
if you're into anomaly detection or under fraud detection,

00:00:11.030 --> 00:00:14.110
you reject the good data points, and you look at the outliers.

00:00:14.110 --> 00:00:17.140
Either way, a really great algorithm that can be looped on

00:00:17.140 --> 00:00:19.740
any machine algorithm is first train,

00:00:19.740 --> 00:00:24.780
then remove the points with the largest error, often called the residual error.

00:00:24.780 --> 00:00:27.410
And for the remaining points, you re-train.

00:00:27.410 --> 00:00:31.029
And if you so wish, you could repeat these removal and re-train procedure.

00:00:31.029 --> 00:00:33.480
But you normally converge to a much,

00:00:33.480 --> 00:00:38.120
much better solution than if you kept all the original data.

00:00:38.120 --> 00:00:42.980
And as I said before, the fraction of data you often remove is something in

00:00:42.980 --> 00:00:46.720
the order of 10%, but it might vary from application to application.

00:00:46.720 --> 00:00:48.980
So correlations, you know about outlier rejection,

00:00:48.980 --> 00:00:53.160
a really important tool in the arsenal of machine learning algorithms.

