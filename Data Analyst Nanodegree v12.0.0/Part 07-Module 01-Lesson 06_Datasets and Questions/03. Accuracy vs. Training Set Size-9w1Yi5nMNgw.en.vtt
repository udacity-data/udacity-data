WEBVTT
Kind: captions
Language: en

00:00:00.160 --> 00:00:02.730
There's a very important thing that I was keeping an eye on

00:00:02.730 --> 00:00:06.770
as I was drawing up my training set of all the persons of interest, and

00:00:06.770 --> 00:00:09.280
that is basically the size of that training set.

00:00:09.280 --> 00:00:11.200
The reason that the size of your training set is so

00:00:11.200 --> 00:00:14.690
important is that it usually has a big effect on the accuracy that you

00:00:14.690 --> 00:00:17.090
are able to achieve with your classifier.

00:00:17.090 --> 00:00:19.220
Let me show you an example of what I mean.

00:00:19.220 --> 00:00:22.100
I once had a problem that I was trying to solve in physics.

00:00:22.100 --> 00:00:26.500
I was using a naive Bayes classifier to try to identify certain types of

00:00:26.500 --> 00:00:32.009
particles, and I had 1,000 events of each class in my training set.

00:00:32.009 --> 00:00:33.870
And my question was basically,

00:00:33.870 --> 00:00:37.410
is this enough events to capture all the trends in the data?

00:00:37.410 --> 00:00:39.770
So here's what I do to answer that question.

00:00:39.770 --> 00:00:45.100
I took my 1,000 events of data and I put about 800 into a training set and

00:00:45.100 --> 00:00:47.410
about 200 into a test set.

00:00:47.410 --> 00:00:53.260
Then I took these 800 and I split them again into 4 times 200.

00:00:53.260 --> 00:00:57.768
So then by recombining different numbers of these slices,

00:00:57.768 --> 00:01:04.632
I could come up with a training set that had 200 events, 400, 600, and 800.

00:01:04.632 --> 00:01:09.670
And then I would always be testing it on the same 200 event testing set.

00:01:09.670 --> 00:01:12.000
And what I found was something that looked like this.

00:01:12.000 --> 00:01:16.200
The maximum accuracy that you can ever get will be 100%.

00:01:16.200 --> 00:01:20.420
And usually in practice, the maximum accuracy that's even

00:01:20.420 --> 00:01:23.380
hypothetically possible will be some number less than 100%.

00:01:23.380 --> 00:01:27.700
And if you don't have enough statistics in your training set, if your training

00:01:27.700 --> 00:01:31.680
set isn't big enough, you'll get an accuracy that's even lower than that.

00:01:31.680 --> 00:01:37.620
So for 200 events, maybe I would get an accuracy that was around 55%.

00:01:37.620 --> 00:01:43.630
However, when I go up to 400 events, my accuracy jumps to let's say 70%.

00:01:43.630 --> 00:01:48.480
So I got another 15 percentage points just by adding more training data.

00:01:48.480 --> 00:01:50.160
I didn't do anything else.

00:01:50.160 --> 00:01:52.170
Then I go up to 600 events.

00:01:52.170 --> 00:01:55.910
Again, my accuracy improves, but probably not as much as before.

00:01:55.910 --> 00:01:58.005
So let's say this is 80% now.

00:01:58.005 --> 00:02:04.280
And then when I add in my last slice of 200 events, I might be at about 82%.

00:02:04.280 --> 00:02:07.790
And what this told me was that there was a trend in the data,

00:02:07.790 --> 00:02:10.800
maybe something like this, that when I looked at it,

00:02:10.800 --> 00:02:13.730
describe to me whether I could expect the accuracy could,

00:02:13.730 --> 00:02:17.860
to get very much better as I increase the size of my training set.

00:02:17.860 --> 00:02:20.120
Obviously, 200 wasn't ideal.

00:02:20.120 --> 00:02:23.060
If I only had a training set that had 200 events in it,

00:02:23.060 --> 00:02:24.840
I couldn't expect to get a very good accuracy.

00:02:24.840 --> 00:02:29.306
By the time I got up to 800, I'm all the way up to 82%.

00:02:29.306 --> 00:02:33.580
And moreover, by looking at this trend, I can see that it's starting to plateau,

00:02:33.580 --> 00:02:38.630
and even if I went out to, let's say, 1,000 events, I might only get up to 83%.

00:02:38.630 --> 00:02:42.810
So there wasn't as much of an advantage to add in another 200 events at

00:02:42.810 --> 00:02:46.750
the end than this first 200 events at the beginning.

00:02:46.750 --> 00:02:50.960
And so, one of my concerns, when trying to identify persons of interest,

00:02:50.960 --> 00:02:54.810
is whether we'd be all the way down here, that we would have so

00:02:54.810 --> 00:02:56.840
few examples of persons of interest,

00:02:56.840 --> 00:03:00.690
especially relative to how many completely innocent people are in our data set.

00:03:00.690 --> 00:03:04.020
But it's very hard to distinguish the patterns that set apart

00:03:04.020 --> 00:03:05.030
the persons of interest.

00:03:06.050 --> 00:03:10.170
Now, in a perfect world, I could maybe try to make a graph like this.

00:03:10.170 --> 00:03:12.600
And if I found out I was down in this corner, I could go out and

00:03:12.600 --> 00:03:14.430
try to find more data.

00:03:14.430 --> 00:03:17.050
In this particular example, that's not really possible, right?

00:03:17.050 --> 00:03:19.540
We only have as many persons of interest as we have.

00:03:19.540 --> 00:03:22.510
It's not like more of them are just going to appear because I want to make

00:03:22.510 --> 00:03:23.360
a better classifier.

00:03:23.360 --> 00:03:28.590
But this is something that's very foremost in my head when I first tackle this

00:03:28.590 --> 00:03:34.500
problem, is how many examples can we get of persons of interest?

00:03:34.500 --> 00:03:37.670
So when I look at this list that I've compiled of the names of the persons of

00:03:37.670 --> 00:03:40.310
interest and I see that we have about 30 people,

00:03:40.310 --> 00:03:42.980
do I think that's enough to proceed with the project?

00:03:42.980 --> 00:03:46.770
And the honest answer is, I don't really know, especially when I first started.

00:03:46.770 --> 00:03:49.940
And there's no good way to know except to try it out.

00:03:49.940 --> 00:03:52.480
But ideally, if you're working in a situation, say,

00:03:52.480 --> 00:03:56.810
the self-driving car, where you have the option of asking a question like this,

00:03:56.810 --> 00:04:00.310
how does the accuracy change with the number of training events, and

00:04:00.310 --> 00:04:04.250
especially if you have the power to go out and collect more data if you need it,

00:04:04.250 --> 00:04:07.670
this can be an extremely useful series of questions to ask.

00:04:07.670 --> 00:04:10.790
And in general, more data is going to

00:04:10.790 --> 00:04:15.360
give you a better result than a super fine-tuned algorithm.

00:04:15.360 --> 00:04:18.730
This is something that surprised a lot of machine learning researchers when they

00:04:18.730 --> 00:04:20.959
first started to discover it many years ago,

00:04:20.959 --> 00:04:24.760
but it's become a truism of machine learning right now, that being able to

00:04:24.760 --> 00:04:29.010
use more data will almost always help out the performance of your algorithm.

