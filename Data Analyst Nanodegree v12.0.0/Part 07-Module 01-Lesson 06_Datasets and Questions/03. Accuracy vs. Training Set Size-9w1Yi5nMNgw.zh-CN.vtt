WEBVTT
Kind: captions
Language: zh-CN

00:00:00.160 --> 00:00:02.730
我准备包括所有相关个人的训练集时

00:00:02.730 --> 00:00:06.769
我一直非常关注一点

00:00:06.769 --> 00:00:09.279
那就是训练集的大小

00:00:09.279 --> 00:00:11.199
训练集大小的重要性在于

00:00:11.199 --> 00:00:14.689
它通常对你能够通过分类器实现的

00:00:14.689 --> 00:00:17.089
精准度具有很大影响

00:00:17.089 --> 00:00:19.219
我举个例子说明我的意思

00:00:19.219 --> 00:00:22.099
我曾经尝试解决一个物理问题

00:00:22.100 --> 00:00:26.500
我幼稚地尝试使用贝叶斯分类器 来确定一些特定粒子

00:00:26.500 --> 00:00:32.009
训练集中的每个类别有 1000 个事件

00:00:32.009 --> 00:00:33.869
我主要的问题是

00:00:33.869 --> 00:00:37.409
这些事件是否足以囊括数据中所有的趋势？

00:00:37.409 --> 00:00:39.769
为解决该问题 我采取了以下措施

00:00:39.770 --> 00:00:45.100
在 1000 个数据事件中 我将大约 800 个放在训练集中

00:00:45.100 --> 00:00:47.410
大约 200 个放在测试集中

00:00:47.409 --> 00:00:53.259
然后我将这 800 个事件再分成 4 份 大约 200 个一份

00:00:53.259 --> 00:00:57.768
然后通过将不同数量的各部分重新组合起来

00:00:57.768 --> 00:01:04.632
我就可以拥有具备 200、400、600 和 800 个事件的训练集

00:01:04.632 --> 00:01:09.670
然后我会一直通过同一个包含 200 个事件的测试集进行测试

00:01:09.670 --> 00:01:12.000
我最后得出的结果如下

00:01:12.000 --> 00:01:16.200
可以得到的最高准确度是 100%

00:01:16.200 --> 00:01:20.420
而通常在实践中 即使假设可能取得的

00:01:20.420 --> 00:01:23.379
最大值也会小于 100%

00:01:23.379 --> 00:01:27.699
如果对训练集统计不足 如果训练集不够大

00:01:27.700 --> 00:01:31.680
那么你得到的准确度甚至会更低

00:01:31.680 --> 00:01:37.620
因此对于 200 个事件 可能得到的准确度大约为 55%

00:01:37.620 --> 00:01:43.630
而上升到 400 个事件时 准确度一下跳跃至比如说 70%

00:01:43.629 --> 00:01:48.479
我只是增加更多训练数据 其他的什么也没做

00:01:48.480 --> 00:01:50.160
准确度就增加了 15%

00:01:50.159 --> 00:01:52.170
然后我又将事件数增加到 600 个

00:01:52.170 --> 00:01:55.909
准确度虽不像上次增幅那么大 但仍有提高

00:01:55.909 --> 00:01:58.004
比如说现在是 80%

00:01:58.004 --> 00:02:04.280
然后当我将最后 200 个事件添加进来时 准确度可能到了 82%

00:02:04.280 --> 00:02:07.790
我从中认识到数据中存在一种趋势

00:02:07.790 --> 00:02:10.800
大约是这种情况 那就是研究数据时

00:02:10.800 --> 00:02:13.730
是不是训练集规模越大

00:02:13.729 --> 00:02:17.859
得到的准确度越高

00:02:17.860 --> 00:02:20.120
很明显 200 这个规模并不理想

00:02:20.120 --> 00:02:23.060
如果训练集中只有 200 个事件

00:02:23.060 --> 00:02:24.840
那么根本无法获得非常好的准确度

00:02:24.840 --> 00:02:29.306
当我增加到 800 个事件时 准确度一路上升到 82%

00:02:29.306 --> 00:02:33.580
还有 通过观察该趋势 我发现准确度开始进入平稳期

00:02:33.580 --> 00:02:38.630
也就是说即使我将事件数提高到 1000 个 准确度可能只上升到 83%

00:02:38.629 --> 00:02:42.810
因此最终额外增加 200 个事件未必就能

00:02:42.810 --> 00:02:46.750
比开始时的 200 个事件好多少

00:02:46.750 --> 00:02:50.960
因此我在尝试确定嫌疑人时 一个顾虑就是

00:02:50.960 --> 00:02:54.810
我们是否要一直这样下去 我们的嫌疑人

00:02:54.810 --> 00:02:56.840
数量很少

00:02:56.840 --> 00:03:00.689
尤其是与数据集中完全没有嫌疑的人相比

00:03:00.689 --> 00:03:04.020
但又很难找出一个模式 能区别出

00:03:04.020 --> 00:03:05.030
嫌疑人

00:03:06.050 --> 00:03:10.170
在理想的世界 我可能尝试制作出这种图表

00:03:10.169 --> 00:03:12.599
如果我发现自己是在这个角落 那么我会展开行动

00:03:12.599 --> 00:03:14.430
并努力发现更多数据

00:03:14.430 --> 00:03:17.050
在该特定示例中 这是不可能的 对吧？

00:03:17.050 --> 00:03:19.540
我们拥有的嫌疑人只有那么多

00:03:19.539 --> 00:03:22.509
而不会只因为我要改善分类器

00:03:22.509 --> 00:03:23.359
而出现更多嫌疑人

00:03:23.360 --> 00:03:28.590
但当我首次应对该问题时 最让我费神的问题就是

00:03:28.590 --> 00:03:34.500
我们能获得多少个嫌疑人？

00:03:34.500 --> 00:03:37.669
当我查看收集的嫌疑人名单

00:03:37.669 --> 00:03:40.309
发现我们有大约 30 个人时

00:03:40.310 --> 00:03:42.979
我是否认为可以继续下去？

00:03:42.979 --> 00:03:46.769
其实我也不知道 尤其是当第一次开始时

00:03:46.770 --> 00:03:49.939
我们别无他法 只能进行尝试

00:03:49.939 --> 00:03:52.479
但理想情况下 如果你工作环境是在 比如

00:03:52.479 --> 00:03:56.810
自动驾驶汽车中 你有机会提出此类问题

00:03:56.810 --> 00:04:00.310
准确度会随着训练集数量发生怎么样的变化

00:04:00.310 --> 00:04:04.250
尤其是如果你有能力去收集更多需要的数据时

00:04:04.250 --> 00:04:07.669
这一系列问题将非常有帮助

00:04:07.669 --> 00:04:10.789
总的来说 更大的数据量

00:04:10.789 --> 00:04:15.359
要比经过精密调整的算法可以提供更好的结果

00:04:15.360 --> 00:04:18.730
许多年前 当机器学习的研究者第一次开始发现

00:04:18.730 --> 00:04:20.959
这一结论时 他们大吃一惊

00:04:20.959 --> 00:04:24.759
但使用更多数据几乎总能帮助算法取得

00:04:24.759 --> 00:04:29.009
更好的效果 这已经是机器学习领域的至理名言

