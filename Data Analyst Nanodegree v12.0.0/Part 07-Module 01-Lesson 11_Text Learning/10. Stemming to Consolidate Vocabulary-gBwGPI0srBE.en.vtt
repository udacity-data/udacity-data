WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:03.040
There's another handy trick that I'm going to teach you now, and

00:00:03.040 --> 00:00:07.380
it has to do with the idea that not all unique words are actually different, or

00:00:07.380 --> 00:00:08.760
not very different anyway.

00:00:08.760 --> 00:00:11.500
Let me show you an example of what I mean.

00:00:11.500 --> 00:00:15.370
Say in my corpus I have a bunch of different versions of the word respond,

00:00:15.370 --> 00:00:19.090
where the meaning changes ever so slightly based on the context or

00:00:19.090 --> 00:00:22.390
based on the part of speech that the word is, but they're all talking about

00:00:22.390 --> 00:00:27.120
basically the same idea, the idea of someone or something responding.

00:00:27.120 --> 00:00:30.810
The idea is that if I naively put these into a bag of words,

00:00:30.810 --> 00:00:33.430
they're all going to show up as different features,

00:00:33.430 --> 00:00:36.330
even though they're all getting at roughly the same idea.

00:00:36.330 --> 00:00:39.400
And this is going to be true for many words in a lot of languages, that they

00:00:39.400 --> 00:00:44.130
have lots of different permutations that mean only slightly different things.

00:00:44.130 --> 00:00:46.010
Luckily for us, there's a way of, sort of,

00:00:46.010 --> 00:00:49.790
bundling these up together, and representing them as a single word, and

00:00:49.790 --> 00:00:53.440
the way that that happens is using an algorithm called a stemmer.

00:00:53.440 --> 00:00:56.500
So if I were to wrap up all these words and put them into a stemmer,

00:00:56.500 --> 00:01:00.060
it would then apply a function to them that would strip them down all

00:01:00.060 --> 00:01:05.129
till they have the same sort of root, which might be something like respon.

00:01:05.129 --> 00:01:10.030
So the idea is not necessarily to make a single word out of this, because,

00:01:10.030 --> 00:01:15.060
of course, respon isn't a word, but it's kind of the root of a word, or the stem

00:01:15.060 --> 00:01:21.110
of a word that can then be used in any of our classifiers or our regressions.

00:01:21.110 --> 00:01:23.860
And we've now taken this five dimensional input space, and

00:01:23.860 --> 00:01:27.125
turned it into one dimension without losing any real information.

00:01:27.125 --> 00:01:31.930
A stemming functions can actually be kind of tricky to implement yourself.

00:01:31.930 --> 00:01:36.150
There are professional linguists and computational linguists who build these

00:01:36.150 --> 00:01:41.280
stemming functions, that best figure out what is the stem of a given word.

00:01:41.280 --> 00:01:44.580
And so, usually what we do in machine learning is we take one of

00:01:44.580 --> 00:01:48.850
these stemmers off the shelf from something like NLTK, or

00:01:48.850 --> 00:01:53.130
some other similar text-processing package, and we just make use of it,

00:01:53.130 --> 00:01:56.202
not necessarily always going into the guts of how it works.

00:01:56.202 --> 00:01:57.580
And then once we've applied the stemming,

00:01:57.580 --> 00:02:01.520
of course, we have a much cleaner body of vocabulary that we can work with.

