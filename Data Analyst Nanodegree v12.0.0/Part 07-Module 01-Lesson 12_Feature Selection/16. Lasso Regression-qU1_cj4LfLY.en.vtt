WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:04.770
So here's the way that a lasso regression can actually make a regression

00:00:04.770 --> 00:00:07.620
simpler in terms of the number of features that it's using.

00:00:07.620 --> 00:00:12.110
So what it does is for features that don't help the regression results enough,

00:00:12.110 --> 00:00:15.820
it can set the coefficient of those features to a very small value,

00:00:15.820 --> 00:00:17.360
potentially to zero.

00:00:17.360 --> 00:00:20.740
So imagine that I have up to four features that are available to me,

00:00:20.740 --> 00:00:24.520
x1 through x4, and I don't know which ones are the most powerful.

00:00:24.520 --> 00:00:26.650
What are the ones that I actually want to be using?

00:00:26.650 --> 00:00:29.120
I don't want to use all of these if I don't have to.

00:00:29.120 --> 00:00:33.380
And that m1 through m4 are the coefficients of regression that I get for

00:00:33.380 --> 00:00:36.918
these features when I actually perform the fit.

00:00:36.918 --> 00:00:40.050
So in an ordinary multivariate regression,

00:00:40.050 --> 00:00:42.310
I would get a result that looks something like this.

00:00:42.310 --> 00:00:45.280
It will use all of the features that I make available to it and

00:00:45.280 --> 00:00:48.184
it will assign to each one a coefficient of regression.

00:00:48.184 --> 00:00:52.100
What lasso regression will do, is it will try adding them in one at a time.

00:00:52.100 --> 00:00:54.720
And if the new feature doesn't improve the fit enough to

00:00:54.720 --> 00:00:58.910
outweigh the penalty term of including that feature, then it won't be added.

00:00:58.910 --> 00:01:01.870
And by not added, it just means that it sets the coefficient to zero.

00:01:01.870 --> 00:01:04.060
So let's suppose that x1 and

00:01:04.060 --> 00:01:07.560
x2 are two features that are really critical to include in our regression.

00:01:07.560 --> 00:01:11.660
They really describe the pattern with a low of power.

00:01:11.660 --> 00:01:14.880
But that x3 and x4 don't improve the fit by very much.

00:01:14.880 --> 00:01:18.610
They might improve it by a little bit, but we could just be fitting to noise.

00:01:18.610 --> 00:01:21.133
Then what will happen is that the coefficients for

00:01:21.133 --> 00:01:23.124
those two features will be set to zero.

00:01:23.124 --> 00:01:27.750
So m3 will be set to 0 and m4 will be set to 0.

00:01:27.750 --> 00:01:30.850
Which effectively means that these two features become irrelevant in the fit,

00:01:30.850 --> 00:01:33.780
and the only thing that's left is x1 and x2,

00:01:33.780 --> 00:01:37.360
the original two features that we wanted to have anyway.

00:01:37.360 --> 00:01:40.730
The power of regularization, the power of having this penalty term for

00:01:40.730 --> 00:01:44.440
the extra features, is that it can automatically do this selection for you.

