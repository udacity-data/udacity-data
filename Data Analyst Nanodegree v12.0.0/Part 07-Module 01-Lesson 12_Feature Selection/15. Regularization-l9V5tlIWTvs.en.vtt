WEBVTT
Kind: captions
Language: en

00:00:00.520 --> 00:00:05.050
One very powerful place that you can use regularization, is in regression.

00:00:05.050 --> 00:00:06.840
Regularization is a method for

00:00:06.840 --> 00:00:11.020
automatically penalizing the extra features that you use in your model.

00:00:11.020 --> 00:00:13.340
So, let me make this a little bit more concrete.

00:00:13.340 --> 00:00:17.140
There's a type of regularized regression called Lasso Regression.

00:00:17.140 --> 00:00:19.560
And, here's the rough formula for the Lasso Regression.

00:00:20.630 --> 00:00:22.350
A regular linear regression would say,

00:00:22.350 --> 00:00:27.640
I just want to minimize the sum of the squared errors in my fit.

00:00:27.640 --> 00:00:30.840
I want to minimize the distance between my fit, and

00:00:30.840 --> 00:00:35.210
any given data point, or the square of that distance.

00:00:35.210 --> 00:00:39.740
What Lasso Regression says is yeah, we want a small sum of squared error.

00:00:39.740 --> 00:00:42.890
But, in addition to minimizing the sum of the squared errors,

00:00:42.890 --> 00:00:45.720
I also want to minimize the number of features that I'm using.

00:00:45.720 --> 00:00:49.390
And, so I'm going to add in a second term here, in which I

00:00:49.390 --> 00:00:53.570
have a penalty parameter, and I have the coefficients of my regression.

00:00:53.570 --> 00:00:58.040
So, this is basically the term that describes how many features I'm using.

00:00:58.040 --> 00:01:00.200
So, here's the result of this formulation.

00:01:00.200 --> 00:01:05.030
When I'm performing my fit, I'm considering both the errors that come from that

00:01:05.030 --> 00:01:09.060
fit, and also the number of features that are being used.

00:01:09.060 --> 00:01:11.390
And, so let's say I'm comparing two different fits,

00:01:11.390 --> 00:01:12.890
that have different number of features in them.

00:01:13.930 --> 00:01:16.150
The one that has more features included,

00:01:16.150 --> 00:01:19.300
will almost certainly have a smaller sum of the squared error.

00:01:19.300 --> 00:01:22.640
because, it can fit more precisely to the points.

00:01:22.640 --> 00:01:25.400
But, I pay a penalty for using that extra feature.

00:01:25.400 --> 00:01:29.300
And, that comes in the second term with the, with the penalty term, and

00:01:29.300 --> 00:01:31.130
the coefficients of regression that I'm going to get for

00:01:31.130 --> 00:01:33.880
that additional feature that I'm using.

00:01:33.880 --> 00:01:36.660
And, so what this is saying is that the gain that I get,

00:01:36.660 --> 00:01:38.740
in terms of the, the precision,

00:01:38.740 --> 00:01:43.630
the goodness of fit of my regression, has to be a bigger gain than the, the loss

00:01:43.630 --> 00:01:47.730
that I take as a result of having that additional feature in my regression.

00:01:49.220 --> 00:01:52.820
So, this precisely formulates, in a mathematical way, the trade off between

00:01:52.820 --> 00:01:58.910
having small errors and having a simpler fit that's using fewer features.

00:01:58.910 --> 00:02:01.040
And, so what Lasso Regression does,

00:02:01.040 --> 00:02:04.970
is it automatically takes into account this penalty parameter.

00:02:04.970 --> 00:02:08.780
And, in so doing, it helps you actually figure out which features that

00:02:08.780 --> 00:02:12.040
are the ones that have the most important effect on your regression.

00:02:12.040 --> 00:02:15.010
And, once it's found those features, it can actually eliminate or

00:02:15.010 --> 00:02:19.500
set to zero, the coefficients for the features that basically don't help

