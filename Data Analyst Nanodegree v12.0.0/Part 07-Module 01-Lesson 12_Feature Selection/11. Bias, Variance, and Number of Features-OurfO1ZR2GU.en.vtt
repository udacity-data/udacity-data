WEBVTT
Kind: captions
Language: en

00:00:00.130 --> 00:00:03.030
Let's dig in a little more to see how the number of features that you use in

00:00:03.030 --> 00:00:06.480
your algorithm is connected to the bias-variance dilemma.

00:00:06.480 --> 00:00:10.190
Remember what we said before is that a high bias algorithm is one

00:00:10.190 --> 00:00:14.310
that pays little attention to the training data and is kind of oversimplified.

00:00:14.310 --> 00:00:16.050
It just does the same thing over and

00:00:16.050 --> 00:00:20.380
over again regardless of what the data might be trying to tell it to do.

00:00:20.380 --> 00:00:23.420
On the other hand an algorithm that's too high variance,

00:00:23.420 --> 00:00:25.310
pays too much attention to the data.

00:00:25.310 --> 00:00:29.690
It doesn't generalize well to new situations that it hasn't quite seen before.

00:00:29.690 --> 00:00:32.270
It's basically just memorizing the training examples.

00:00:32.270 --> 00:00:35.310
And as soon as it gets a new example or a new data point that's not

00:00:35.310 --> 00:00:39.280
exactly like one of the training examples, it doesn't really know what to do.

00:00:39.280 --> 00:00:42.330
Another way to think about this is that it's overfitting to the data.

00:00:42.330 --> 00:00:46.010
Another thing that would be fair to say is that high biased algorithms tend to

00:00:46.010 --> 00:00:48.290
have high error on the training set.

00:00:48.290 --> 00:00:51.116
So in the case of a regression, for example, would be mean a low

00:00:51.116 --> 00:00:55.580
r-squared value or a large sum of the squared residual errors.

00:00:55.580 --> 00:01:00.620
High variance on the other hand might have a very, very good fit to the training

00:01:00.620 --> 00:01:06.500
data but a bad fit to the test data because it's not generalizing very well.

00:01:06.500 --> 00:01:09.170
So as soon as you give it something new it starts to run into

00:01:09.170 --> 00:01:10.740
problems right away.

00:01:10.740 --> 00:01:13.410
You usually expect to do a little bit better on the training set than you do

00:01:13.410 --> 00:01:14.660
on the test set.

00:01:14.660 --> 00:01:18.650
But high variance means that your doing much better on the training set.

00:01:18.650 --> 00:01:21.201
But high variance is when your over fitting to the training set,

00:01:21.201 --> 00:01:23.450
you get much worse performance on the test set.

00:01:23.450 --> 00:01:24.830
So, here's a quiz question for you.

00:01:24.830 --> 00:01:29.530
Let's suppose you have an algorithm that's only using a few features in it.

00:01:29.530 --> 00:01:32.520
Out of, say, very many that you have available to you.

00:01:32.520 --> 00:01:35.190
So maybe one or two out of dozens, potentially.

00:01:35.190 --> 00:01:37.670
Without knowing anything more about the exact problem or

00:01:37.670 --> 00:01:41.530
the exact features that you're using, do you think that this would

00:01:41.530 --> 00:01:46.270
be more inclined to be a high bias situation, or a high variance situation?

00:01:46.270 --> 00:01:47.590
Click which one sounds right to you.

