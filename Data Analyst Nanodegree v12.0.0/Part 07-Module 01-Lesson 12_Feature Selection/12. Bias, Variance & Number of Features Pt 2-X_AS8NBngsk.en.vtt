WEBVTT
Kind: captions
Language: en

00:00:00.420 --> 00:00:04.019
In the case where you're using lots of features to get the most tightly fitting

00:00:04.019 --> 00:00:05.050
regression or,

00:00:05.050 --> 00:00:09.180
or a classifier that you can, that's a classic high variance situation.

00:00:09.180 --> 00:00:12.240
You want to be careful that you're not overfitting to

00:00:12.240 --> 00:00:14.140
the data when you're doing this.

00:00:14.140 --> 00:00:17.300
And so, another way to frame this whole discussion about the number of features

00:00:17.300 --> 00:00:22.460
that you should be using can be phrased in terms of the bias variance dilemma,

00:00:22.460 --> 00:00:24.400
or how many features should you be using so

00:00:24.400 --> 00:00:26.580
that you're balancing these two concerns.

00:00:26.580 --> 00:00:29.620
You want to have the accurate description that comes with having enough

00:00:29.620 --> 00:00:33.110
variance to your model, you want it to be able to fit your data in a,

00:00:33.110 --> 00:00:36.140
in an accurate and true way.

00:00:36.140 --> 00:00:36.760
But you want to do it

00:00:36.760 --> 00:00:39.620
with the minimum number of features that's needed to do that.

00:00:41.040 --> 00:00:44.940
So there's this tradeoff between sort of the goodness of the fit and

00:00:44.940 --> 00:00:46.150
the simplicity of the fit,

00:00:46.150 --> 00:00:49.730
the number of features that you have to use to achieve that goodness of fit.

00:00:49.730 --> 00:00:53.440
And so what that means is you want to fit an algorithm with few features.

00:00:53.440 --> 00:00:57.820
But using the case of a regression as a large r squared or

00:00:57.820 --> 00:01:01.310
conversely a low sum of the squared residual errors.

00:01:01.310 --> 00:01:03.127
This is the sweet spot that you want to find.

