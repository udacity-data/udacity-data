WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:02.130
Here's another way of thinking about this trade-off.

00:00:02.130 --> 00:00:05.780
We want to balance the errors that our regression gives us,

00:00:05.780 --> 00:00:09.140
with the number of features that are required to get those errors.

00:00:09.140 --> 00:00:12.460
We're going to try to maximize the quality of the model that we're making.

00:00:12.460 --> 00:00:15.120
I haven't totally defined this yet, but let me fill out this graph, and

00:00:15.120 --> 00:00:17.230
you'll get a sense for what I mean by this.

00:00:17.230 --> 00:00:20.030
Let's suppose I have only one feature in my model.

00:00:20.030 --> 00:00:22.848
Then, I would say that the likelihood is pretty high that I'm

00:00:22.848 --> 00:00:24.050
under-fitting my data

00:00:24.050 --> 00:00:27.070
that I have something that's very high-bias.

00:00:27.070 --> 00:00:29.520
So, I'd say the quality of the model might be all right.

00:00:29.520 --> 00:00:32.100
But, not as good as it could be.

00:00:32.100 --> 00:00:35.610
Now, let's suppose that I allow a little bit more complexity into my model.

00:00:35.610 --> 00:00:37.310
I allow to use three features.

00:00:37.310 --> 00:00:40.330
And, I happen to pick three features that are good at

00:00:40.330 --> 00:00:41.730
describing the patterns in the data.

00:00:42.750 --> 00:00:44.810
Then, my error is going to go down,

00:00:44.810 --> 00:00:48.490
because it's my model is now fitting my data in a, in a more precise way.

00:00:49.500 --> 00:00:50.850
But it's not too complex yet.

00:00:50.850 --> 00:00:52.900
I'm only using three features, and I've checked each one of them.

00:00:52.900 --> 00:00:55.220
So, the quality of my model has now gone up.

00:00:55.220 --> 00:00:57.200
So, then I say, well, that's pretty sweet.

00:00:57.200 --> 00:01:00.540
I can improve my model, just by adding more features into it.

00:01:00.540 --> 00:01:02.290
So, let me try to add in a whole bunch of features.

00:01:02.290 --> 00:01:03.670
I'm going to jump up to 15.

00:01:03.670 --> 00:01:06.610
And, I'm hoping that that'll put the quality of my model,

00:01:06.610 --> 00:01:09.720
say somewhere up here, at the maximal value that it can have.

00:01:09.720 --> 00:01:14.070
What I'll actually find though, is that I'm starting to become susceptible to

00:01:14.070 --> 00:01:16.800
this over-fitting, high variance type regime.

00:01:16.800 --> 00:01:20.750
So, maybe the quality of my model has actually become a little bit

00:01:20.750 --> 00:01:24.660
lower than the quality that it was at, at three features.

00:01:24.660 --> 00:01:26.190
Because, now I'm starting to over fit,

00:01:26.190 --> 00:01:30.030
where before I was under-fitting with one, I'm now over fitting with 15.

00:01:30.030 --> 00:01:33.150
And, if I were to go all the way out to 20,

00:01:33.150 --> 00:01:37.210
then I might even be doing worse than if I had only one input feature.

00:01:37.210 --> 00:01:40.480
And, so you could imagine that if I sort of filled out this,

00:01:40.480 --> 00:01:45.280
this graph for all of the possible number of features that I could get, there

00:01:45.280 --> 00:01:51.100
would be some best point where the quality of my model is going to be maximized.

00:01:51.100 --> 00:01:54.530
So, a reasonable thing to think, and what we'll talk about for the next few

00:01:54.530 --> 00:01:59.450
videos, is how we can mathematically define what this arc might be so

00:01:59.450 --> 00:02:01.830
that we can find this maximum point of it.

00:02:01.830 --> 00:02:04.500
And, this is a process that's called regularization.

00:02:04.500 --> 00:02:08.479
It's an automatic form of feature selection that some algorithms can

00:02:08.479 --> 00:02:13.330
do completely on their own, that they can trade off between the precision,

00:02:13.330 --> 00:02:18.530
the goodness of fit, the very low errors, and, the complexity of,

00:02:18.530 --> 00:02:20.670
of fitting on lots and lots of different features.

00:02:20.670 --> 00:02:24.940
So, let me show you an example of how regularization works in regressions.

