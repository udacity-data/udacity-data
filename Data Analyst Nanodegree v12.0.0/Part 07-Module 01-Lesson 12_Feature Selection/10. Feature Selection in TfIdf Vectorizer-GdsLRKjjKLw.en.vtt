WEBVTT
Kind: captions
Language: en

00:00:00.180 --> 00:00:02.150
So in order to make this a little more concrete,

00:00:02.150 --> 00:00:03.670
which might help you understand what I mean.

00:00:03.670 --> 00:00:07.670
I'm going to show you some actual example from some code that you've actually

00:00:07.670 --> 00:00:10.640
been running already in this lesson, maybe without even knowing it.

00:00:10.640 --> 00:00:12.040
So think back to the first couple of

00:00:12.040 --> 00:00:15.240
lessons where we were learning about supervised classification algorithms and

00:00:15.240 --> 00:00:18.410
we were trying to identify emails by their authors.

00:00:18.410 --> 00:00:23.030
Now, what I wanted you to do was to focus on the naive base classifier or

00:00:23.030 --> 00:00:26.290
the SVM or whatever and just leave the pre-processing to me.

00:00:26.290 --> 00:00:28.680
I would take care of, of reading in the emails and

00:00:28.680 --> 00:00:31.420
getting them into a format that was good for you.

00:00:31.420 --> 00:00:34.040
Now what I'm doing is I'm taking you into the code where I

00:00:34.040 --> 00:00:36.440
actually did that pre-processing step.

00:00:36.440 --> 00:00:39.200
So that you can see that there's, there's really no black magic there.

00:00:39.200 --> 00:00:43.300
And that you would be able to do this completely on your own in the future.

00:00:43.300 --> 00:00:45.660
Let me take you into that code and show you what I'm doing.

00:00:45.660 --> 00:00:46.780
So I load in the data.

00:00:46.780 --> 00:00:51.000
I put it in my TfidfVectorizer, which you're very familiar with by now.

00:00:51.000 --> 00:00:53.240
And then there's another step right here.

00:00:53.240 --> 00:00:54.510
Select percentile.

00:00:54.510 --> 00:00:57.380
I make a selector and I select a percentile.

00:00:57.380 --> 00:01:02.180
And what this step actually does, is it gets rid of a lot of the features.

00:01:02.180 --> 00:01:03.570
I know that this is text data.

00:01:03.570 --> 00:01:06.450
And that it's going to have thousands or perhaps tens of

00:01:06.450 --> 00:01:09.770
thousands of features because of just the large vocabularies that we have.

00:01:09.770 --> 00:01:14.010
And I also know that a lot of those words are going to be irrelevant.

00:01:14.010 --> 00:01:16.340
So stop words is one example of irrelevant words.

00:01:16.340 --> 00:01:19.100
But there's probably a lot of words in there that aren't stop words, but

00:01:19.100 --> 00:01:21.510
still just don't have a lot of information in them.

00:01:21.510 --> 00:01:24.750
They're not going to help me figure out who the author is very well.

00:01:24.750 --> 00:01:29.440
And so what I've done here in this step, where I select a percentile,

00:01:29.440 --> 00:01:32.500
is I actually say, get rid of a bunch of the features.

00:01:32.500 --> 00:01:37.020
You go in and figure out for each feature individually how good of a job that

00:01:37.020 --> 00:01:40.850
feature does at telling you the two people's emails apart from each other.

00:01:40.850 --> 00:01:43.870
And only accept the best 10% of the features for

00:01:43.870 --> 00:01:45.630
me to then use in my classifier.

00:01:45.630 --> 00:01:49.010
So what I've done here is I've kind of sliced off that very top layer.

00:01:49.010 --> 00:01:51.030
The features that seem to have the most information.

00:01:51.030 --> 00:01:54.220
And I'm focusing on those when I make my classifier.

00:01:54.220 --> 00:01:58.240
Now there's one other place where I did some feature reduction as well.

00:01:58.240 --> 00:02:00.650
And you might not always want to mix these two together.

00:02:00.650 --> 00:02:03.990
But I want to send you on a little bit of a treasure hunt to find it.

00:02:03.990 --> 00:02:06.310
So I've told you about select percentile.

00:02:06.310 --> 00:02:08.630
This is something that's available to me in

00:02:08.630 --> 00:02:11.270
the sklearn.feature_selection module.

00:02:11.270 --> 00:02:14.480
But there's also some feature selection that I can get

00:02:14.480 --> 00:02:18.300
when I'm performing the TfidfVectorization of my data.

00:02:18.300 --> 00:02:21.240
When I'm actually taking the words in my corpus and

00:02:21.240 --> 00:02:23.480
putting them into my Tfidf matrix.

00:02:24.850 --> 00:02:29.120
So what I want you to do is look up the documentation for this function,

00:02:29.120 --> 00:02:31.260
the TfidfVectorizer.

00:02:31.260 --> 00:02:33.810
And take a look at the arguments that I'm passing to it.

00:02:33.810 --> 00:02:36.450
One of these arguments does something kind of interesting.

00:02:36.450 --> 00:02:38.930
It says that if there's a word that's quite frequent,

00:02:38.930 --> 00:02:41.330
that's something that we want to ignore as well.

00:02:41.330 --> 00:02:46.560
By quite frequent, what I mean is that it occurs in many of the documents.

00:02:46.560 --> 00:02:50.850
So for example, if you have a word that occurs in every single document, there's

00:02:50.850 --> 00:02:56.200
an argument in here that will actually remove it while you're making your Tfidf.

00:02:56.200 --> 00:02:57.950
So this is a little of a tricky quiz,

00:02:57.950 --> 00:03:01.170
because I haven't told you exactly which argument is doing this.

00:03:01.170 --> 00:03:03.120
Although a quick Google and

00:03:03.120 --> 00:03:05.630
the sklearn documentation should make it pretty clear.

00:03:05.630 --> 00:03:10.980
But the question that I'll ask you in the quiz is what this threshold is, for

00:03:10.980 --> 00:03:14.200
this particular Tfidf to ignore the word.

00:03:14.200 --> 00:03:16.566
So, does a word get tossed out,

00:03:16.566 --> 00:03:20.960
if it occurs in 10% of the documents, 50% or 90%?

