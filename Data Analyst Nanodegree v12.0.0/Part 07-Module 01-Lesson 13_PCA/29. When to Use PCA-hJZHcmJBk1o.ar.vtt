WEBVTT
Kind: captions
Language: ar

00:00:00.200 --> 00:00:04.780
‫بعد أن اكتسبتم خلفية جيدة عن ماهية PCA، وكيفية استخدامه،

00:00:04.780 --> 00:00:07.600
‫المثال الأخير الذي سأوضحه لكم، بعد قليل،

00:00:07.600 --> 00:00:11.830
‫هو أحد أروع الأمثلة على محل استخدام PCA على أرض الواقع.

00:00:11.830 --> 00:00:14.260
‫لكن قبل أن نخوض في هذا، دعونا نتوقف قليلاً

00:00:14.260 --> 00:00:15.990
‫للحديث عن الوقت الذي تحتاج عنده إلى استخدام PCA.

00:00:16.990 --> 00:00:19.330
‫ومتى يكون نهجًا مناسبًا؟

00:00:19.330 --> 00:00:23.460
‫الحالة الأولى هي إذا كنا نريد الوصول إلى الميزات الكامنة التي نظن أنها

00:00:23.460 --> 00:00:25.540
‫تظهر في الأنماط الموجودة في البيانات.

00:00:25.540 --> 00:00:28.370
‫وقد يكون الهدف كله من ما نحاول القيام به هو معرفة ما إذا

00:00:28.370 --> 00:00:30.010
‫كانت هناك ميزات كامنة أم لا.

00:00:30.010 --> 00:00:33.480
‫بعبارة أخرى، نريد فقط أن نعرف حجم المكون الرئيسي الأول.

00:00:33.480 --> 00:00:35.270
‫وأحد الأمثلة الأخرى على هذا الأمر هو أمر مثل،

00:00:35.270 --> 00:00:38.240
‫هل تستطيعون قياس من هم الشخصيات الهامة في شركة Enron؟

00:00:38.240 --> 00:00:41.240
‫الحالة الثانية هي بالطبع تقليل الأبعاد.

00:00:41.240 --> 00:00:44.570
‫حيث توجد العديد من الأمور التي يمكن لتحليل PCA القيام بها لمساعدتنا فيما يخص ذلك.

00:00:44.570 --> 00:00:48.300
‫أولها، أن بإمكانها مساعدتنا على تصور بيانات عالية الأبعاد.

00:00:48.300 --> 00:00:51.914
‫إذن بالطبع عندما نرسم مخططًا مبعثرًا يتاح لنا بعدين فقط،

00:00:51.914 --> 00:00:55.598
‫ولكن في الكثير من الأحيان يكون لدينا أكثر من ميزتين.

00:00:55.598 --> 00:00:58.997
‫إذن، يوجد نوع من الصعوبة يتمثل في كيفية تمثيل ثلاثة أو أربعة أو عدد

00:00:58.997 --> 00:01:03.200
‫أكبر من الميزات لنقطة بيانات إذا كان لدينا بعدان فقط يمكن رسمها بهما.

00:01:04.290 --> 00:01:07.157
‫ما يمكن أن نقوم به، هو رسم إسقاط لها على أول

00:01:07.157 --> 00:01:11.070
‫مكونين رئيسيين، ثم تخطيط ذلك ورسم النقطة المبعثرة هذه فحسب.

00:01:11.070 --> 00:01:14.270
‫وقد يكون تصور الأمور التي تشبه نظام مجموعات k-means

00:01:14.270 --> 00:01:15.620
‫أسهل بكثير.

00:01:15.620 --> 00:01:18.410
‫حيث يظل بإمكاننا تمثيل معظم المعلومات الموجودة بالبيانات ولكن

00:01:18.410 --> 00:01:21.280
‫الآن أصبح بإمكاننا رسمها في هذين البعدين.

00:01:21.280 --> 00:01:22.870
‫الأمر الآخر الذي يمكن لتحليل PCA المساعدة فيه

00:01:22.870 --> 00:01:24.940
‫هو إذا كنا نشك في وجود تشويش في البيانات.

00:01:24.940 --> 00:01:27.380
‫وسنجد تشويشًا في جميع البيانات تقريبًا.

00:01:27.380 --> 00:01:30.900
‫نأمل أن يمثل المكون الرئيسي الأول أو الثاني، وهم أقوى مكونات رئيسية لدينا،

00:01:30.900 --> 00:01:32.830
‫الأنماط الفعلية الموجودة في البيانات.

00:01:32.830 --> 00:01:36.010
‫وأن تمثل المكونات الرئيسية الأصغر

00:01:36.010 --> 00:01:38.380
‫مجرد اختلافات مشوشة لهذه الأنماط.

00:01:38.380 --> 00:01:41.460
‫لذا فبالتخلص من المكونات الرئيسية غير الهامة،

00:01:41.460 --> 00:01:42.399
‫نكون قد تخلصنا من هذا التشويش.

00:01:43.440 --> 00:01:47.020
‫الحالة الأخيرة، والتي سنستخدمها كمثال حتى نهاية هذا الدرس،

00:01:47.020 --> 00:01:50.920
‫هي استخدام PCA كمعالجة مسبقة قبل استخدام خوارزمية أخرى،

00:01:50.920 --> 00:01:53.160
‫كمهمة انحدار أو تصنيف.

00:01:53.160 --> 00:01:55.940
‫وكما تعلمون، إذا كان لدينا أبعاد عالية جدًا،

00:01:55.940 --> 00:01:59.710
‫وإذا كانت لدينا خوارزمية تصنيف مثلاً،

00:01:59.710 --> 00:02:01.760
‫فقد تكون الخوارزمية ذات تباين عالٍ جدًا،

00:02:01.760 --> 00:02:04.360
‫وقد ينتهي بها المطاف إلى ملاءمة التشويش إلى البيانات.

00:02:04.360 --> 00:02:05.800
‫وقد ينتهي بها المطاف إلى العمل ببطء شديد.

00:02:05.800 --> 00:02:08.740
‫هناك العديد من الأمور التي قد تحدث عندما يكون لدينا أبعاد إدخال عالية جدًا

00:02:08.740 --> 00:02:10.639
‫بالنسبة لبعض هذه الخوارزميات.

00:02:10.639 --> 00:02:13.660
‫ولكن بالطبع قد تعمل الخوارزمية بصورة جيدة جدًا لمعالجة المسألة الموجودة لدينا.

00:02:13.660 --> 00:02:17.470
‫أحد الأمور التي يمكن القيام بها هي استخدام PCA لتقليل الأبعاد في

00:02:17.470 --> 00:02:18.840
‫ميزات الإدخال لدينا.

00:02:18.840 --> 00:02:21.830
‫بحيث تعمل خوارزمية التصنيف مثلاً الموجودة لدينا بصورة أفضل.

00:02:22.960 --> 00:02:25.030
‫وهذا هو المثال الذي سنتناوله فيما يلي.

00:02:25.030 --> 00:02:27.320
‫ويسمى eigenfaces،

00:02:27.320 --> 00:02:30.790
‫وهو عبارة عن أسلوب لتطبيق PCA على صور الأشخاص.

00:02:30.790 --> 00:02:34.930
‫وهذا فضاء عالي الأبعاد للغاية، حيث تحتوي الصورة على

00:02:34.930 --> 00:02:36.610
‫عدد هائل من وحدات البكسل.

00:02:36.610 --> 00:02:39.890
‫ولكن فلنفترض أننا نريد التعرف على الشخص الموجود في الصورة.

00:02:39.890 --> 00:02:43.700
‫وسنقوم بإجراء نوع ما من التعرف على الوجوه، أو ما شابه.

00:02:43.700 --> 00:02:47.550
‫إذن باستخدام PCA يمكننا تقليل أبعاد الإدخال العالية

00:02:47.550 --> 00:02:50.190
‫لتصبح أقل بعشرة أضعاف مثلاً.

00:02:50.190 --> 00:02:53.930
‫ثم تغذية ذلك إلى جهاز SVM، والذي يمكنه القيام بعملية التصنيف الفعلية

00:02:53.930 --> 00:02:55.670
‫لمحاولة التعرف على الشخص الموجود في الصورة.

00:02:55.670 --> 00:02:57.870
‫وبهذا تصبح المدخلات هي المكونات الرئيسية،

00:02:57.870 --> 00:03:00.840
‫بدلاً من أن تكون هي وحدات البكسل أو الصور الأصلية.

00:03:00.840 --> 00:03:02.850
‫دعوني أعرض لكم هذا المثال وستفهمون ما أعنيه.

