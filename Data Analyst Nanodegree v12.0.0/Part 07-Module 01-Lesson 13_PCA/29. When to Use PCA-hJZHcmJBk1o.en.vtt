WEBVTT
Kind: captions
Language: en

00:00:00.200 --> 00:00:04.780
So now you should have a good intuition for what PCA is, and how to use it.

00:00:04.780 --> 00:00:07.600
The last example I'm going to give you, starting very soon,

00:00:07.600 --> 00:00:11.830
is one of the coolest examples of where PCA is actually used in the wild.

00:00:11.830 --> 00:00:14.260
But before we get into that, let me just pause and, and

00:00:14.260 --> 00:00:15.990
talk about when you want to use PCA.

00:00:16.990 --> 00:00:19.330
When is it an appropriate approach?

00:00:19.330 --> 00:00:23.460
The first one is if you want access to latent features that you think might be

00:00:23.460 --> 00:00:25.540
showing up in the patterns in your data.

00:00:25.540 --> 00:00:28.370
Maybe the entire point of what you're trying to do is figure out if

00:00:28.370 --> 00:00:30.010
there's a latent feature.

00:00:30.010 --> 00:00:33.480
In other words you just want to know the size of the first principal component.

00:00:33.480 --> 00:00:35.270
An example of this might be something like,

00:00:35.270 --> 00:00:38.240
can you measure who the big shots are at Enron.

00:00:38.240 --> 00:00:41.240
The second one of course is dimensionality reduction.

00:00:41.240 --> 00:00:44.570
There's a number of things that PCA can do to help you out on this front.

00:00:44.570 --> 00:00:48.300
The first is that it can help you visualize high-dimensional data.

00:00:48.300 --> 00:00:51.914
So, of course when you're drawing a scatterplot you only have two dimensions

00:00:51.914 --> 00:00:55.598
that are available to you, but many times you'll have more than two features.

00:00:55.598 --> 00:00:58.997
So there's kind a struggle of how to represent three or four or many

00:00:58.997 --> 00:01:03.200
numbers about a data point if you only have two dimensions in which to draw it.

00:01:04.290 --> 00:01:07.157
And so what you can do, is you can project it down to the first two

00:01:07.157 --> 00:01:11.070
principal components and just plot that, and just draw that scatter point.

00:01:11.070 --> 00:01:14.270
And then things like, k-means clustering might be a lot easier for

00:01:14.270 --> 00:01:15.620
you to visualize.

00:01:15.620 --> 00:01:18.410
You're still capturing most of the information in the data but

00:01:18.410 --> 00:01:21.280
now you can draw it with those, with those two dimensions.

00:01:21.280 --> 00:01:22.870
Another thing the PCA can help with is if you

00:01:22.870 --> 00:01:24.940
suspect that there's noise in your data.

00:01:24.940 --> 00:01:27.380
And, in almost all data there will be noise.

00:01:27.380 --> 00:01:30.900
The hope is that the first or the second, your strongest principal components,

00:01:30.900 --> 00:01:32.830
are capturing the actual patterns in the data.

00:01:32.830 --> 00:01:36.010
And the smaller principle components are just representing

00:01:36.010 --> 00:01:38.380
noisy variations about those patterns.

00:01:38.380 --> 00:01:41.460
So by throwing away the less important principle components,

00:01:41.460 --> 00:01:42.399
you're getting rid of that noise.

00:01:43.440 --> 00:01:47.020
The last one, and what we'll use as our example for the rest of this lesson,

00:01:47.020 --> 00:01:50.920
is using PCA as pre-processing before you use another algorithm, so

00:01:50.920 --> 00:01:53.160
a regression or a classification task.

00:01:53.160 --> 00:01:55.940
As you know if you have very high dimensionality, and

00:01:55.940 --> 00:01:59.710
if you have a complex, say, classification algorithm.

00:01:59.710 --> 00:02:01.760
The algorithm can be very high variance,

00:02:01.760 --> 00:02:04.360
it can end up fitting to noise in the data.

00:02:04.360 --> 00:02:05.800
It can end up running really slow.

00:02:05.800 --> 00:02:08.740
There are lots of things that can happen when you have very high input

00:02:08.740 --> 00:02:10.639
dimensionality with some of these algorithms.

00:02:10.639 --> 00:02:13.660
But, of course, the algorithm might work really well for the problem at hand.

00:02:13.660 --> 00:02:17.470
So one of the things you can do is use PCA to reduce the dimensionality of

00:02:17.470 --> 00:02:18.840
your input features.

00:02:18.840 --> 00:02:21.830
So that then your, say, classification algorithm works better.

00:02:22.960 --> 00:02:25.030
And this is the example that we'll do next.

00:02:25.030 --> 00:02:27.320
It's something called eigenfaces, and

00:02:27.320 --> 00:02:30.790
it's a method of applying PCA to pictures of people.

00:02:30.790 --> 00:02:34.930
So this is very high dimensionality space, you have many,

00:02:34.930 --> 00:02:36.610
many pixels in the picture.

00:02:36.610 --> 00:02:39.890
But say, you want to identify who is pictured in the image.

00:02:39.890 --> 00:02:43.700
You're running some kind of facial identification, or what have you.

00:02:43.700 --> 00:02:47.550
So with PCA you can reduce the very high input dimensionality,

00:02:47.550 --> 00:02:50.190
into something that's maybe a factor of ten lower.

00:02:50.190 --> 00:02:53.930
And feed this into an SVM, which can then do the actual classification of

00:02:53.930 --> 00:02:55.670
trying to figure out who's pictured.

00:02:55.670 --> 00:02:57.870
So now the inputs, instead of being the original pixels or

00:02:57.870 --> 00:03:00.840
the images, are the principal components.

00:03:00.840 --> 00:03:02.850
So let me show you this example and you'll see what I mean.

