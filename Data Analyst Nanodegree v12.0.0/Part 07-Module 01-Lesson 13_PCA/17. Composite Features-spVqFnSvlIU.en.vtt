WEBVTT
Kind: captions
Language: en

00:00:00.130 --> 00:00:02.480
So here, are the two crucial ingredients.

00:00:02.480 --> 00:00:07.000
I have many features available to me, but I hypothesize that there's

00:00:07.000 --> 00:00:10.320
a smaller number of features that are actually driving the patterns in the data.

00:00:10.320 --> 00:00:13.810
And then, what I do is I take that knowledge and I try making a composite

00:00:13.810 --> 00:00:17.080
feature that more directly probes the underlying phenomenon.

00:00:17.080 --> 00:00:20.190
These composite features are what we'll be talking about in this lesson.

00:00:20.190 --> 00:00:22.050
They're called principle components.

00:00:22.050 --> 00:00:24.030
And it's an extremely powerful algorithm.

00:00:24.030 --> 00:00:24.610
In this lesson,

00:00:24.610 --> 00:00:27.970
we'll mostly talk about it in the context of dimensionality reduction,

00:00:27.970 --> 00:00:30.560
how you can use principle component analysis.

00:00:30.560 --> 00:00:33.710
Or PCA, to bring down the dimensionality of your features to

00:00:33.710 --> 00:00:35.930
turn a whole bunch of features into just a few.

00:00:35.930 --> 00:00:39.230
It's also a very powerful standalone method in its own right for

00:00:39.230 --> 00:00:41.930
unsupervised learning, and I'll give you some of my favorite examples of

00:00:41.930 --> 00:00:43.590
that in the latter half of this lesson.

00:00:43.590 --> 00:00:44.750
It's going to be really cool.

00:00:44.750 --> 00:00:46.860
So, here's an example of what PCA is,

00:00:46.860 --> 00:00:49.990
transforming from square footage plus number of rooms into a single

00:00:49.990 --> 00:00:53.140
variable that roughly tracks the size of the house.

00:00:53.140 --> 00:00:55.150
&gt;&gt; So, let's suppose this is my training data,

00:00:55.150 --> 00:00:59.840
I have the square footage on the x axis and the number of rooms on the y axis.

00:00:59.840 --> 00:01:03.650
Now, what I'm going to do is I'm going to draw in the principle component.

00:01:03.650 --> 00:01:05.230
It will look something like this.

00:01:05.230 --> 00:01:09.390
I know this looks like a regression at this point, but it's not, and here's why.

00:01:09.390 --> 00:01:11.700
With the regression, what you're trying to do is you're trying to

00:01:11.700 --> 00:01:15.990
predict an output variable with respect to the value of an input variable.

00:01:15.990 --> 00:01:18.100
Here, we're not really trying to predict anything.

00:01:18.100 --> 00:01:22.140
We're trying to come up with a direction in the data that we can project our

00:01:22.140 --> 00:01:25.780
data onto while losing a minimal amount of information.

00:01:25.780 --> 00:01:27.550
So, here's what that means.

00:01:27.550 --> 00:01:31.800
Once I've found my principal component, once I have the direction of this,

00:01:31.800 --> 00:01:35.450
of this vector, of this line, and just go with me for now, that it exists.

00:01:35.450 --> 00:01:37.480
We'll come back to how we find it in a little bit.

00:01:37.480 --> 00:01:40.710
Then, what I'm going to do is I'm going to take all of my data points.

00:01:40.710 --> 00:01:43.470
And, I'm going to go through a process that's called projection.

00:01:43.470 --> 00:01:46.810
And, what a projection means, it's a little bit of a physical term almost.

00:01:46.810 --> 00:01:49.600
Imagine that I have a light and the light is sort of

00:01:49.600 --> 00:01:53.550
shining down perpendicular to the surface of this principle component.

00:01:53.550 --> 00:01:55.640
Then, you can imagine that all of the data points are going to be

00:01:55.640 --> 00:01:59.320
sort of casting shadows down onto like a piece of paper if I put it there.

00:01:59.320 --> 00:02:01.530
Now, my principle component looks like this.

00:02:01.530 --> 00:02:03.598
So I start out with two dimensional data.

00:02:03.598 --> 00:02:06.690
But, once I've projected it down onto the principle component,

00:02:06.690 --> 00:02:09.560
it's in one dimension and it's going to look something like this.

00:02:09.560 --> 00:02:14.150
Now, instead of being the blue circles, my data is going to be the black xs.

00:02:14.150 --> 00:02:16.130
I've turned it into a one dimensional distribution.

00:02:16.130 --> 00:02:21.070
I've taken the feature that I've made in this diagram and sort of turned it so

00:02:21.070 --> 00:02:22.260
that it's lying flat.

00:02:22.260 --> 00:02:23.219
What's it going to look like?

