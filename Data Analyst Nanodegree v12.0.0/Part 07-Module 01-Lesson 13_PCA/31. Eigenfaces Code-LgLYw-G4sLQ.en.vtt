WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:03.450
And now for the next few videos I'm going to show you an actually coded up

00:00:03.450 --> 00:00:06.620
example of PCA as applied to facial recognition.

00:00:06.620 --> 00:00:10.507
This is taken and adapted from an example on the sklearn documentation, so

00:00:10.507 --> 00:00:14.219
I'll include a link to the original code so you can go in and take a look at it,

00:00:14.219 --> 00:00:17.253
but I want to walk you through some of the most important parts.

00:00:17.253 --> 00:00:19.376
So the first thing that the code does is,

00:00:19.376 --> 00:00:23.930
import a set of pictures of famous world leaders from about 10 to 15 years ago.

00:00:23.930 --> 00:00:27.530
The next thing that it does is it splits it into a training and testing set.

00:00:27.530 --> 00:00:31.280
Very good, but then this block right here is where the PCA actually happens.

00:00:31.280 --> 00:00:34.380
You'll see in this example, and sometimes in the literature that PCA is

00:00:34.380 --> 00:00:37.880
also called eigenfaces when it's applied to facial recognition.

00:00:37.880 --> 00:00:41.230
So in this line here we actually see the PCA being created,

00:00:41.230 --> 00:00:44.020
it's called RandomizedPCA in sklearn.

00:00:44.020 --> 00:00:46.700
And then it's also being fit to the training data.

00:00:46.700 --> 00:00:49.850
Then what this line does here is it asks for the eigenfaces.

00:00:49.850 --> 00:00:53.320
The eigenfaces are basically the principle components of the face data.

00:00:53.320 --> 00:00:56.220
So it takes the pca.components, and then it reshapes them.

00:00:56.220 --> 00:00:58.340
because right now they're just strings of numbers, and

00:00:58.340 --> 00:01:03.250
it wants to turn those back into squares so that they look like pictures.

00:01:03.250 --> 00:01:07.168
You can also see that the number of principal components that are being used in

00:01:07.168 --> 00:01:08.443
this example is 150.

00:01:08.443 --> 00:01:12.348
And if you look at the example code on the sklearn documentation page,

00:01:12.348 --> 00:01:16.692
you'll see that the original dimensionality of these pictures is over 1,800.

00:01:16.692 --> 00:01:19.716
So we've gone from 1,800 features down to 150,

00:01:19.716 --> 00:01:21.800
a compression factor of more than 10.

00:01:21.800 --> 00:01:25.410
Then the last thing that I do with PCA is I transform my data.

00:01:25.410 --> 00:01:26.870
When I perform the fit command I'm

00:01:26.870 --> 00:01:29.435
just figuring out what the principle components are.

00:01:29.435 --> 00:01:32.640
It's these transform commands where I actually take my data and

00:01:32.640 --> 00:01:35.110
transform them into the principle components representation.

00:01:35.110 --> 00:01:37.830
And the rest of this code is creating an SVM.

00:01:37.830 --> 00:01:40.270
Remember the SVC is what its called in sklearn,

00:01:40.270 --> 00:01:41.970
a support vector classifier.

00:01:41.970 --> 00:01:45.080
They do a little bit of fancy footwork here to figure out exactly what

00:01:45.080 --> 00:01:49.140
parameters of the support vector machine they want to use.

00:01:49.140 --> 00:01:51.480
And then using the principle components as the features,

00:01:51.480 --> 00:01:55.770
they try to identify in the test set who appears in a given picture.

00:01:55.770 --> 00:01:58.240
Now I'll show you what this looks like when I run this code.

00:01:58.240 --> 00:01:59.990
First thing that it does is it prints out a doc string,

00:01:59.990 --> 00:02:01.460
just tells me what's going on.

00:02:01.460 --> 00:02:02.530
And then some information about the data set.

00:02:02.530 --> 00:02:04.420
So I have 1,288 samples,

00:02:04.420 --> 00:02:09.060
with 1,850 features available to me in the input feature space.

00:02:09.060 --> 00:02:10.740
Seven different people are appearing and

00:02:10.740 --> 00:02:14.940
then we use 150 eigenfaces from the 966 faces in our training set.

00:02:14.940 --> 00:02:18.320
The next thing that appears is a list of the people in our data set, and

00:02:18.320 --> 00:02:19.780
how often we get them correct.

00:02:19.780 --> 00:02:23.130
Precision, recall, f1-score and support are things that are sort of

00:02:23.130 --> 00:02:25.680
related to the accuracy, their evaluation matrix.

00:02:25.680 --> 00:02:28.170
We'll talk a lot about those in a coming lesson.

00:02:28.170 --> 00:02:31.670
You can see that in general though, it's getting things correct.

00:02:31.670 --> 00:02:33.880
Roughly 60% to almost 90% of the time.

00:02:33.880 --> 00:02:38.350
So even though we've reduced our dimensionality by a factor of ten,

00:02:38.350 --> 00:02:40.370
we're still having really good performance.

00:02:40.370 --> 00:02:43.100
Another thing that's really cool that I love about this example is they

00:02:43.100 --> 00:02:45.460
actually show you the eigenfaces.

00:02:45.460 --> 00:02:47.580
So this is the first principle component of our data, and

00:02:47.580 --> 00:02:50.180
this is the second principle component, the third one.

00:02:50.180 --> 00:02:53.670
So this image here represents the maximal variation that it

00:02:53.670 --> 00:02:55.800
sees across all the data.

00:02:55.800 --> 00:02:58.830
It's a little bit hard to understand exactly what that means in this example.

00:02:58.830 --> 00:03:01.020
It would be better if it said something like,

00:03:01.020 --> 00:03:05.920
how far apart the eyes are, or whether the person wears glasses or not.

00:03:05.920 --> 00:03:08.990
Instead, what we get are these kind of ghost images.

00:03:08.990 --> 00:03:13.838
But then using these composite images together as features in an SVM,

00:03:13.838 --> 00:03:17.832
it can be very powerful in predicting who's face we see.

00:03:17.832 --> 00:03:18.380
And then last but

00:03:18.380 --> 00:03:22.130
not least, there's a little printout that gives you 12 representative faces and

00:03:22.130 --> 00:03:25.010
the algorithms best guess about who it is, and the actual answer.

00:03:25.010 --> 00:03:27.100
So you can see it doesn't always get them correct.

00:03:27.100 --> 00:03:30.900
This one right here I think this is Tony Blair, but it's actually George W Bush,

00:03:30.900 --> 00:03:33.010
but on most of them it's getting it correct, pretty cool.

