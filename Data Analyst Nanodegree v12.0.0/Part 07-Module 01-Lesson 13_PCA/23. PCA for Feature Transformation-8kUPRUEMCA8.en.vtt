WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:03.000
Now I think you have a good idea of what PCA generally does.

00:00:03.000 --> 00:00:04.730
And I want to take a step back and

00:00:04.730 --> 00:00:08.150
talk about it as a general algorithm for future transformation.

00:00:08.150 --> 00:00:11.040
So remember what we started with was four features that we

00:00:11.040 --> 00:00:13.660
manually split up into two categories.

00:00:13.660 --> 00:00:16.572
We said square footage and number of rooms are related to size, and

00:00:16.572 --> 00:00:19.420
the safety and schools are related to the neighborhood.

00:00:19.420 --> 00:00:22.255
Then we applied PCA to each of those independently and

00:00:22.255 --> 00:00:24.430
we came up with that single feature.

00:00:24.430 --> 00:00:26.350
Then we can put those features into say,

00:00:26.350 --> 00:00:29.360
a regression and come up with a house price.

00:00:29.360 --> 00:00:31.560
You could also imagine that these would be good inputs for

00:00:31.560 --> 00:00:34.680
a classifier if you're trying to answer some slightly different question.

00:00:34.680 --> 00:00:38.180
But there's a little bit of a problem here and maybe it's been bothering you.

00:00:38.180 --> 00:00:42.670
And that's this step right here that we manually took our input features and

00:00:42.670 --> 00:00:45.910
we used our human intuition to figure out how we should group them.

00:00:45.910 --> 00:00:50.250
And the problem with this is that this manual process is not scalable.

00:00:50.250 --> 00:00:52.342
Let's suppose we're trying to use PCA for

00:00:52.342 --> 00:00:55.948
something like facial recognition where the inputs might be thousands or

00:00:55.948 --> 00:00:59.000
tens of thousands or even millions of pixels in a picture.

00:00:59.000 --> 00:01:02.260
There's no way that we could inspect all of those pixels by hand and

00:01:02.260 --> 00:01:06.120
try to split them up according to what we think might be the combinations that

00:01:06.120 --> 00:01:09.770
make the most sense, and then apply PCA to each of those individually.

00:01:09.770 --> 00:01:11.580
But here's the magic of PCA.

00:01:11.580 --> 00:01:14.120
I can take a second strategy with this.

00:01:14.120 --> 00:01:18.570
What I can do is I can put all four of these features into PCA together, and

00:01:18.570 --> 00:01:21.850
it can automatically combine them into new features and

00:01:21.850 --> 00:01:24.380
rank the relative powers of those new features.

00:01:24.380 --> 00:01:27.330
So if we have the case where we have two latent features that

00:01:27.330 --> 00:01:31.030
are driving most of the variation in the data, PCA will pick those out and

00:01:31.030 --> 00:01:34.080
it will call them the first and the second principal components.

00:01:34.080 --> 00:01:35.290
Where the first principle component,

00:01:35.290 --> 00:01:38.780
the one that has the most effect, might be something like neighborhood, and

00:01:38.780 --> 00:01:41.520
the second principle component might have something like size.

00:01:41.520 --> 00:01:44.960
It's a little bit harder to make these interpretations now because what can and

00:01:44.960 --> 00:01:49.070
does happen is that the first principal component will be an admixture that

00:01:49.070 --> 00:01:53.130
has little bits and pieces from potentially all of the features.

00:01:53.130 --> 00:01:57.520
But this is a very powerful unsupervised learning technique that you can use to

00:01:57.520 --> 00:02:01.070
fundamentally understand the latent features in your data.

00:02:01.070 --> 00:02:03.440
If you knew nothing about housing prices at all,

00:02:03.440 --> 00:02:07.220
PCA would still be able to give you an insight like there are two things that

00:02:07.220 --> 00:02:09.389
seem to drive the house of prices in general.

00:02:09.389 --> 00:02:11.930
It would still be up to you to figure out that they're the neighborhood and

00:02:11.930 --> 00:02:15.650
the size, but in addition to doing dimensionality reduction now,

00:02:15.650 --> 00:02:19.060
you're also learning something fundamentally important about the patterns of

00:02:19.060 --> 00:02:20.550
the variation in your data.

