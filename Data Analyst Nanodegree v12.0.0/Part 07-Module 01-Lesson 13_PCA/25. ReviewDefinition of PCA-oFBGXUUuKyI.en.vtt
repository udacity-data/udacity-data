WEBVTT
Kind: captions
Language: en

00:00:00.320 --> 00:00:03.120
If you're still with me at this point, great job.

00:00:03.120 --> 00:00:06.930
I think PCA is one of the trickiest topics in machine learning, so

00:00:06.930 --> 00:00:10.180
if you're still sort of struggling to wrap your head around it, don't worry.

00:00:10.180 --> 00:00:10.980
That's normal.

00:00:10.980 --> 00:00:15.010
So let me take this opportunity though to just review PCA briefly at a high

00:00:15.010 --> 00:00:18.870
level and give you kind of a working definition that you can use in the future.

00:00:18.870 --> 00:00:22.695
PCA is a systematized way to transform input features into their

00:00:22.695 --> 00:00:23.968
principal components.

00:00:23.968 --> 00:00:26.630
Then those principal components are available to you

00:00:26.630 --> 00:00:29.730
to use instead of your original input features.

00:00:29.730 --> 00:00:33.990
So you use them as new features in your regression or classification task.

00:00:33.990 --> 00:00:38.450
The principal components are defined as the directions in the data that maximize

00:00:38.450 --> 00:00:41.860
the variance, which has the effect of minimizing the information loss when you

00:00:41.860 --> 00:00:46.070
perform a projection or a compression down onto those principal components.

00:00:46.070 --> 00:00:48.570
You can also rank order the principal components.

00:00:48.570 --> 00:00:52.060
The more variance you have of the data along a given principal component,

00:00:52.060 --> 00:00:54.390
the higher that principal component is ranked.

00:00:54.390 --> 00:00:57.820
So the one that has the most variance will be the first principal component,

00:00:57.820 --> 00:01:00.150
second will be the second principal component, and so on.

00:01:00.150 --> 00:01:03.800
Another thing that we should point out is that the principal components are all

00:01:03.800 --> 00:01:05.770
perpendicular to each other in a sense, so

00:01:05.770 --> 00:01:09.470
the second principal component is mathematically guaranteed to not overlap at

00:01:09.470 --> 00:01:11.120
all with the first principal component.

00:01:11.120 --> 00:01:14.690
And the third will not overlap with the first through the second, and so on.

00:01:14.690 --> 00:01:17.860
So you can treat them as independent features in a sense.

00:01:17.860 --> 00:01:21.640
And last, there's a maximum number of principal components you can find.

00:01:21.640 --> 00:01:24.980
It's equal to the number of input features that you had in your dataset.

00:01:24.980 --> 00:01:28.600
Usually, you'll only use the first handful of principal components, but

00:01:28.600 --> 00:01:31.140
you could go all the way out and use the maximum number.

00:01:31.140 --> 00:01:33.120
In that case though, you're not really gaining anything.

00:01:33.120 --> 00:01:35.780
You're just representing your features in a different way.

00:01:35.780 --> 00:01:38.720
So the PCA won't give you the wrong answer, but it doesn't give you any

00:01:38.720 --> 00:01:42.710
advantages over just using the original input features if you're using all of

00:01:42.710 --> 00:01:46.230
the principal components together in a regression or classification task.

