WEBVTT
Kind: captions
Language: zh-CN

00:00:00.820 --> 00:00:04.820
— 你又学完了一个算法 恭喜你！你的第三个机器算法 决策树

00:00:04.820 --> 00:00:06.230
你已经学完了所有相关的内容

00:00:06.230 --> 00:00:07.810
— 我们已经学习了很多关于决策树的内容

00:00:07.810 --> 00:00:11.180
现在 让我们将这一新知识的部分内容

00:00:11.180 --> 00:00:13.190
放到挑选分类器时的注意事项中

00:00:13.190 --> 00:00:16.055
— 决策树非常易于使用 完整的决策树结构会很漂亮

00:00:16.055 --> 00:00:18.155
从某种程度上来说

00:00:18.155 --> 00:00:20.111
它们能以图形化方式很好地剖析数据

00:00:20.111 --> 00:00:23.563
这比支持向量机的结果要容易理解得多

00:00:23.563 --> 00:00:24.821
但是它们也存在一些不足

00:00:24.821 --> 00:00:25.750
— 没错

00:00:25.750 --> 00:00:27.585
决策树存在的一个缺点就是

00:00:27.585 --> 00:00:29.090
容易过拟合

00:00:29.090 --> 00:00:31.880
尤其对于具有包含大量特征的数据时

00:00:31.880 --> 00:00:34.580
复杂的决策树可能会过拟合数据

00:00:34.580 --> 00:00:37.600
所以你需要谨慎对待决策树的参数

00:00:37.600 --> 00:00:39.320
仔细调整参数 以避免过拟合

00:00:39.320 --> 00:00:41.300
— 是的 对于节点上只有单个数据点的决策树

00:00:41.300 --> 00:00:42.950
它的结构看起来会让人抓狂

00:00:42.950 --> 00:00:44.360
这几乎肯定是发生了过拟合

00:00:44.360 --> 00:00:47.150
所以 测量决策树的准确率是非常重要的

00:00:47.150 --> 00:00:50.300
你需要在适当的时候停止决策树的生长

00:00:50.300 --> 00:00:52.420
— 决策树还有一个很棒的地方

00:00:52.420 --> 00:00:55.180
就是可以通过所谓的集成方法

00:00:55.180 --> 00:00:58.360
从决策树出发构建更大规模的分类器

00:00:58.360 --> 00:00:59.060
下节课中

00:00:59.060 --> 00:01:02.310
你将有机会独立探索一个算法

00:01:02.310 --> 00:01:05.600
我们为你提供的一些选项中就包括

00:01:05.600 --> 00:01:06.280
集成方法

00:01:06.280 --> 00:01:07.550
所以 如果你对这种方法感兴趣 也就是从一个分类器出发构建另一个分类器

00:01:07.550 --> 00:01:11.413
请继续观看下节课

00:01:11.413 --> 00:01:15.820
— 是的 你是最棒的机器学习研究者 继续看下节课吧

00:01:15.820 --> 00:01:16.540
— 我们开始吧

