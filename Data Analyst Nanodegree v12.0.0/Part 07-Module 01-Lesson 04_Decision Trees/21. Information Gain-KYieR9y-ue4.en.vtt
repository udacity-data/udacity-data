WEBVTT
Kind: captions
Language: en

00:00:00.360 --> 00:00:03.440
While I'm sure you're really excited about entropy right now,

00:00:03.440 --> 00:00:05.820
let's take this back to decision trees and

00:00:05.820 --> 00:00:09.250
see how entropy actually affects how a decision tree draws its boundaries.

00:00:10.680 --> 00:00:15.100
That involves a new piece of vocabulary, information gain.

00:00:16.290 --> 00:00:18.920
Information gain is defined as the entropy of the parent

00:00:20.130 --> 00:00:23.110
minus the weighted average of the entropy of the children that

00:00:23.110 --> 00:00:24.980
would result if you split that parent.

00:00:24.980 --> 00:00:30.300
The decision tree algorithm will maximize information gain.

00:00:31.830 --> 00:00:35.130
So this is how it will choose which feature to make a split on.

00:00:35.130 --> 00:00:37.760
And in cases where the feature has many different values that it can take,

00:00:37.760 --> 00:00:40.340
this will help it figure out where to make that split.

00:00:40.340 --> 00:00:43.640
It is going to try to maximize the information gain.

00:00:43.640 --> 00:00:44.480
Let's do an example.

