WEBVTT
Kind: captions
Language: en

00:00:00.150 --> 00:00:00.960
So Katie, what do you think?

00:00:00.960 --> 00:00:02.580
Which ones are the right answers here?

00:00:02.580 --> 00:00:06.130
&gt;&gt; The right answers, so the, the ones that do need rescaled features will

00:00:06.130 --> 00:00:09.110
be the SVM and the k-means clustering.

00:00:09.110 --> 00:00:13.540
&gt;&gt; So both and, support vector machines in, in, in k-means clustering, you're

00:00:13.540 --> 00:00:16.360
really trading off one dimension to the other when you calculate the distance.

00:00:16.360 --> 00:00:19.160
So take, for example, support vector machines.

00:00:19.160 --> 00:00:23.030
And you look at the separation line that maximizes distance.

00:00:23.030 --> 00:00:24.870
In there, you calculate a distance.

00:00:24.870 --> 00:00:29.610
And that distance calculation, trade-offs one dimension against the other.

00:00:29.610 --> 00:00:33.600
So we make one twice as big as the other, it counts for twice as much.

00:00:33.600 --> 00:00:36.335
The same is true, coincidentally, for

00:00:36.335 --> 00:00:39.360
k-means clustering, where you have a cluster center.

00:00:39.360 --> 00:00:44.420
And you compute the distance of the cluster center, to all the data points.

00:00:44.420 --> 00:00:46.840
And that distance itself has exactly the same characterization.

00:00:46.840 --> 00:00:50.510
If you make one variable twice as big, it's going to count for twice as much.

00:00:50.510 --> 00:00:52.545
So, as a result, support vector machines and

00:00:52.545 --> 00:00:56.700
k-means both are affected by feature rescaling.

00:00:56.700 --> 00:00:59.550
So, Katie, tell me about the decision trees and linear regression.

00:00:59.550 --> 00:01:01.180
Why aren't they included?

00:01:01.180 --> 00:01:04.519
Decision trees aren't going to give you a diagonal line like that, right?

00:01:04.519 --> 00:01:07.910
They're going to give you a series of vertical and horizontal lines.

00:01:07.910 --> 00:01:09.200
So there's no trade off.

00:01:09.200 --> 00:01:11.860
You just, make a cut in one direction, and then a cut in another.

00:01:11.860 --> 00:01:14.830
So, you don't have to worry about what's going on in one dimension,

00:01:14.830 --> 00:01:16.370
when you're doing something with the other one.

00:01:16.370 --> 00:01:19.260
&gt;&gt; So if you squeeze this area little area over here to half the size,

00:01:19.260 --> 00:01:22.820
because you rescale the feature where the image line lies.

00:01:22.820 --> 00:01:24.300
Well, it'll lie in a different place but

00:01:24.300 --> 00:01:26.700
the separation is chronologically the same as before.

00:01:26.700 --> 00:01:27.480
It scales with it,

00:01:27.480 --> 00:01:31.910
so there's no trade-off between these two different variables.

00:01:31.910 --> 00:01:33.760
And how about, linear regression?

00:01:33.760 --> 00:01:36.250
&gt;&gt; Something similar happens in linear regression.

00:01:36.250 --> 00:01:37.980
Remember that in linear regression,

00:01:37.980 --> 00:01:41.360
each of our features is going to have a coefficient that's associated with it.

00:01:41.360 --> 00:01:44.070
And that coefficient and that feature always go together.

00:01:44.070 --> 00:01:46.380
What's going on with feature A doesn't effect anything with

00:01:46.380 --> 00:01:47.750
the coefficient of feature B.

00:01:47.750 --> 00:01:49.360
So they're separated in the same way.

00:01:49.360 --> 00:01:53.980
&gt;&gt; In fact, if you were to double the variable scale of one specific variable,

00:01:53.980 --> 00:01:55.900
that feature will just become half as big.

00:01:55.900 --> 00:01:57.750
And the output would be exactly the same as before.

00:01:57.750 --> 00:02:00.340
So it's really interesting to see, and for some algorithms,

00:02:00.340 --> 00:02:05.180
rescaling is really a potential if we can use it, for others, don't even bother.

